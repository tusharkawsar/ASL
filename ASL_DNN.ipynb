{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division,absolute_import\n",
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Let's define the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Dropout,MaxPool2D,Activation,Flatten,BatchNormalization\n",
    "from keras.optimizers import Adam,Adadelta,RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 100, 100)      640       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 100, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 100, 100)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 98, 98)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 98, 98)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 49, 49)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 49, 49)        196       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 47, 47)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128, 47, 47)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 47, 47)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 45, 45)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128, 45, 45)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 22, 22)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 22, 22)       88        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 256, 20, 20)       295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256, 20, 20)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256, 20, 20)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 18, 18)       590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256, 18, 18)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 9, 9)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 9, 9)         36        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 512, 7, 7)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 5, 5)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512, 5, 5)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 2, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512, 2, 2)         8         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 5,867,538\n",
      "Trainable params: 5,867,374\n",
      "Non-trainable params: 164\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "feature_layers = [\n",
    "    Conv2D(64,3,\n",
    "           padding='same',\n",
    "           input_shape=(1,100,100)),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64,3),\n",
    "    Activation('elu'),\n",
    "    MaxPool2D(pool_size=2),\n",
    "  \n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Conv2D(128,3),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(128,3),\n",
    "    Activation('elu'),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    \n",
    "\n",
    "    Conv2D(256,3),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(256,3),\n",
    "    Activation('elu'),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    \n",
    "    \n",
    "    Conv2D(512,3),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(512,3),\n",
    "    Activation('elu'),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    \n",
    "    \n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(512),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    \n",
    "    Dense(256),\n",
    "    Activation('elu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "#model building\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "model.summary()\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 1, 100, 100) (4800, 10)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load('ASL_Train.npz')\n",
    "trainX,TrainY = dat['arr_0'],dat['arr_1']\n",
    "trainY = utils.np_utils.to_categorical(TrainY,10)\n",
    "trainX = trainX/255\n",
    "trainX = trainX.astype('float32')\n",
    "trainX = trainX.reshape((trainX.shape[0],1,100,100)).astype('float32')\n",
    "print(trainX.shape,trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1, 100, 100) (200, 10)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load('ASL_Test.npz')\n",
    "testX,TestY = dat['arr_0'],dat['arr_1']\n",
    "testY = utils.np_utils.to_categorical(TestY,10)\n",
    "testX = testX/255\n",
    "testX = testX.astype('float32')\n",
    "testX = testX.reshape((testX.shape[0],1,100,100)).astype('float32')\n",
    "print(testX.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 2.8215 - acc: 0.2125Epoch 00000: acc improved from -inf to 0.21313, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 56s - loss: 2.8154 - acc: 0.2131 - val_loss: 2.9441 - val_acc: 0.1350\n",
      "Epoch 2/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 1.0963 - acc: 0.6365Epoch 00001: acc improved from 0.21313 to 0.63667, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 52s - loss: 1.0947 - acc: 0.6367 - val_loss: 5.3984 - val_acc: 0.1000\n",
      "Epoch 3/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8880Epoch 00002: acc improved from 0.63667 to 0.88771, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.3347 - acc: 0.8877 - val_loss: 1.9122 - val_acc: 0.5300\n",
      "Epoch 4/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9456Epoch 00003: acc improved from 0.88771 to 0.94563, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.1671 - acc: 0.9456 - val_loss: 0.6246 - val_acc: 0.8050\n",
      "Epoch 5/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9680Epoch 00004: acc improved from 0.94563 to 0.96792, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.1020 - acc: 0.9679 - val_loss: 0.2416 - val_acc: 0.9500\n",
      "Epoch 6/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9830Epoch 00005: acc improved from 0.96792 to 0.98313, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0527 - acc: 0.9831 - val_loss: 0.2227 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9856Epoch 00006: acc improved from 0.98313 to 0.98521, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0376 - acc: 0.9852 - val_loss: 0.1198 - val_acc: 0.9650\n",
      "Epoch 8/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9895Epoch 00007: acc improved from 0.98521 to 0.98958, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0314 - acc: 0.9896 - val_loss: 0.0535 - val_acc: 0.9900\n",
      "Epoch 9/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9927Epoch 00008: acc improved from 0.98958 to 0.99271, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0217 - acc: 0.9927 - val_loss: 0.1263 - val_acc: 0.9650\n",
      "Epoch 10/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9937Epoch 00009: acc improved from 0.99271 to 0.99375, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0201 - acc: 0.9938 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 11/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9969Epoch 00010: acc improved from 0.99375 to 0.99688, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0120 - acc: 0.9969 - val_loss: 0.1826 - val_acc: 0.9500\n",
      "Epoch 12/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9950Epoch 00011: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0145 - acc: 0.9950 - val_loss: 0.0661 - val_acc: 0.9800\n",
      "Epoch 13/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9962Epoch 00012: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 0.0117 - acc: 0.9963 - val_loss: 0.4817 - val_acc: 0.8900\n",
      "Epoch 14/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9977Epoch 00013: acc improved from 0.99688 to 0.99771, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0300 - val_acc: 0.9850\n",
      "Epoch 15/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9969Epoch 00014: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0196 - val_acc: 0.9900\n",
      "Epoch 16/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9994Epoch 00015: acc improved from 0.99771 to 0.99938, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0796 - val_acc: 0.9900\n",
      "Epoch 17/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9977Epoch 00016: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0058 - acc: 0.9977 - val_loss: 0.1207 - val_acc: 0.9850\n",
      "Epoch 18/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996Epoch 00017: acc improved from 0.99938 to 0.99958, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0411 - val_acc: 0.9850\n",
      "Epoch 19/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985Epoch 00018: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0383 - val_acc: 0.9900\n",
      "Epoch 20/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9990Epoch 00019: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0323 - val_acc: 0.9900\n",
      "Epoch 21/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990Epoch 00020: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0663 - val_acc: 0.9850\n",
      "Epoch 22/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996Epoch 00021: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0113 - val_acc: 0.9950\n",
      "Epoch 23/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 6.5056e-04 - acc: 1.0000Epoch 00022: acc improved from 0.99958 to 1.00000, saving model to ASL-new-normal-weights.h5\n",
      "4800/4800 [==============================] - 53s - loss: 6.4738e-04 - acc: 1.0000 - val_loss: 0.0433 - val_acc: 0.9850\n",
      "Epoch 24/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9994Epoch 00023: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0114 - val_acc: 0.9950\n",
      "Epoch 25/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992Epoch 00024: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 26/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9983Epoch 00025: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 0.0039 - acc: 0.9983 - val_loss: 0.0391 - val_acc: 0.9950\n",
      "Epoch 27/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996Epoch 00026: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0431 - val_acc: 0.9950\n",
      "Epoch 28/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 5.3595e-04 - acc: 0.9998Epoch 00027: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 5.3327e-04 - acc: 0.9998 - val_loss: 0.0172 - val_acc: 0.9900\n",
      "Epoch 29/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 1.5074e-04 - acc: 1.0000Epoch 00028: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 1.4999e-04 - acc: 1.0000 - val_loss: 0.0286 - val_acc: 0.9900\n",
      "Epoch 30/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 1.0634e-04 - acc: 1.0000Epoch 00029: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 1.0583e-04 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 0.9900\n",
      "Epoch 31/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9992Epoch 00030: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0014 - acc: 0.9992 - val_loss: 0.0809 - val_acc: 0.9750\n",
      "Epoch 32/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998Epoch 00031: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0087 - val_acc: 0.9950\n",
      "Epoch 33/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 4.7446e-04 - acc: 0.9998Epoch 00032: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 4.7222e-04 - acc: 0.9998 - val_loss: 0.0183 - val_acc: 0.9850\n",
      "Epoch 34/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 4.0002e-04 - acc: 0.9998Epoch 00033: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 3.9808e-04 - acc: 0.9998 - val_loss: 0.0447 - val_acc: 0.9850\n",
      "Epoch 35/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9983Epoch 00034: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0053 - acc: 0.9983 - val_loss: 0.2615 - val_acc: 0.9650\n",
      "Epoch 36/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 8.6449e-04 - acc: 0.9996Epoch 00035: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 8.6026e-04 - acc: 0.9996 - val_loss: 1.4583e-05 - val_acc: 1.0000\n",
      "Epoch 37/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 2.3881e-04 - acc: 0.9998Epoch 00036: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 2.3780e-04 - acc: 0.9998 - val_loss: 0.0182 - val_acc: 0.9950\n",
      "Epoch 38/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 4.4423e-04 - acc: 0.9998Epoch 00037: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 4.4201e-04 - acc: 0.9998 - val_loss: 0.0101 - val_acc: 0.9950\n",
      "Epoch 39/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996Epoch 00038: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0204 - val_acc: 0.9900\n",
      "Epoch 40/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 1.0007e-04 - acc: 1.0000Epoch 00039: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 9.9573e-05 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9950\n",
      "Epoch 41/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 3.2374e-05 - acc: 1.0000Epoch 00040: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 3.2217e-05 - acc: 1.0000 - val_loss: 0.0330 - val_acc: 0.9900\n",
      "Epoch 42/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 4.9363e-05 - acc: 1.0000Epoch 00041: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 4.9118e-05 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 0.9900\n",
      "Epoch 43/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 3.5941e-04 - acc: 0.9998Epoch 00042: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 3.5761e-04 - acc: 0.9998 - val_loss: 0.0788 - val_acc: 0.9850\n",
      "Epoch 44/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 1.1703e-04 - acc: 1.0000Epoch 00043: acc did not improve\n",
      "4800/4800 [==============================] - 52s - loss: 1.1812e-04 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9950\n",
      "Epoch 45/200\n",
      "4776/4800 [============================>.] - ETA: 0s - loss: 3.8849e-05 - acc: 1.0000Epoch 00044: acc did not improve\n",
      "4800/4800 [==============================] - 53s - loss: 3.8656e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 46/200\n",
      " 168/4800 [>.............................] - ETA: 51s - loss: 8.1214e-07 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e92007da08c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#now let's make the data Augmentation\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from os.path import isfile\n",
    "data_aug_weight_file = 'ASL-new-normal-weights.h5'\n",
    "\n",
    "if (isfile(data_aug_weight_file)):\n",
    "    model.load_weights(data_aug_weight_file)\n",
    "\n",
    "checkpoint = ModelCheckpoint(data_aug_weight_file, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir='./logs-ASL-normal', histogram_freq=0,write_graph=True, write_images=True)\n",
    "callbacks_list=[checkpoint,tensorboard]\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=24,epochs=200,verbose=1, validation_data=(testX, testY),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 3.1856 - acc: 0.1102Epoch 00000: val_acc improved from -inf to 0.10000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 63s - loss: 3.1855 - acc: 0.1102 - val_loss: 2.5515 - val_acc: 0.1000\n",
      "Epoch 2/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.6778 - acc: 0.1071Epoch 00001: val_acc improved from 0.10000 to 0.11000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 2.6779 - acc: 0.1073 - val_loss: 2.3539 - val_acc: 0.1100\n",
      "Epoch 3/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.5710 - acc: 0.1144Epoch 00002: val_acc improved from 0.11000 to 0.14500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 2.5709 - acc: 0.1142 - val_loss: 2.2565 - val_acc: 0.1450\n",
      "Epoch 4/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.4941 - acc: 0.1150Epoch 00003: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 2.4945 - acc: 0.1148 - val_loss: 2.3452 - val_acc: 0.1250\n",
      "Epoch 5/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.4528 - acc: 0.1207Epoch 00004: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 2.4526 - acc: 0.1204 - val_loss: 2.2712 - val_acc: 0.1250\n",
      "Epoch 6/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.4044 - acc: 0.1278Epoch 00005: val_acc improved from 0.14500 to 0.15000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 2.4046 - acc: 0.1277 - val_loss: 2.2526 - val_acc: 0.1500\n",
      "Epoch 7/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.3696 - acc: 0.1230Epoch 00006: val_acc improved from 0.15000 to 0.15000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 2.3694 - acc: 0.1231 - val_loss: 2.2559 - val_acc: 0.1500\n",
      "Epoch 8/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.3273 - acc: 0.1307Epoch 00007: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 2.3272 - acc: 0.1306 - val_loss: 2.3350 - val_acc: 0.1300\n",
      "Epoch 9/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.2748 - acc: 0.1660Epoch 00008: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 2.2755 - acc: 0.1656 - val_loss: 2.8327 - val_acc: 0.1150\n",
      "Epoch 10/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.2145 - acc: 0.1818Epoch 00009: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 2.2144 - acc: 0.1821 - val_loss: 2.2607 - val_acc: 0.1200\n",
      "Epoch 11/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.1311 - acc: 0.2129Epoch 00010: val_acc improved from 0.15000 to 0.22500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 2.1312 - acc: 0.2131 - val_loss: 2.1099 - val_acc: 0.2250\n",
      "Epoch 12/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 2.0272 - acc: 0.2441Epoch 00011: val_acc improved from 0.22500 to 0.25500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 2.0270 - acc: 0.2444 - val_loss: 2.0069 - val_acc: 0.2550\n",
      "Epoch 13/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.9322 - acc: 0.2825Epoch 00012: val_acc improved from 0.25500 to 0.28000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.9306 - acc: 0.2831 - val_loss: 2.0607 - val_acc: 0.2800\n",
      "Epoch 14/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.8096 - acc: 0.3265Epoch 00013: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 1.8094 - acc: 0.3269 - val_loss: 3.1183 - val_acc: 0.2600\n",
      "Epoch 15/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.7262 - acc: 0.3616Epoch 00014: val_acc improved from 0.28000 to 0.40500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.7257 - acc: 0.3621 - val_loss: 1.6767 - val_acc: 0.4050\n",
      "Epoch 16/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.5886 - acc: 0.4010Epoch 00015: val_acc improved from 0.40500 to 0.62500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.5874 - acc: 0.4015 - val_loss: 0.9690 - val_acc: 0.6250\n",
      "Epoch 17/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.4758 - acc: 0.4480Epoch 00016: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 1.4742 - acc: 0.4485 - val_loss: 1.6992 - val_acc: 0.4700\n",
      "Epoch 18/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.3727 - acc: 0.4873Epoch 00017: val_acc improved from 0.62500 to 0.70000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 63s - loss: 1.3722 - acc: 0.4873 - val_loss: 0.7927 - val_acc: 0.7000\n",
      "Epoch 19/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.3033 - acc: 0.5205Epoch 00018: val_acc improved from 0.70000 to 0.71500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.3034 - acc: 0.5206 - val_loss: 0.7032 - val_acc: 0.7150\n",
      "Epoch 20/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.2321 - acc: 0.5372Epoch 00019: val_acc did not improve\n",
      "480/480 [==============================] - 62s - loss: 1.2326 - acc: 0.5367 - val_loss: 0.7321 - val_acc: 0.7100\n",
      "Epoch 21/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.1835 - acc: 0.5620Epoch 00020: val_acc improved from 0.71500 to 0.79000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.1837 - acc: 0.5621 - val_loss: 0.5515 - val_acc: 0.7900\n",
      "Epoch 22/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.1416 - acc: 0.5875Epoch 00021: val_acc improved from 0.79000 to 0.79500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.1412 - acc: 0.5873 - val_loss: 0.5050 - val_acc: 0.7950\n",
      "Epoch 23/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 1.0537 - acc: 0.6132Epoch 00022: val_acc did not improve\n",
      "480/480 [==============================] - 62s - loss: 1.0536 - acc: 0.6129 - val_loss: 0.5687 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9926 - acc: 0.6388Epoch 00023: val_acc improved from 0.79500 to 0.80000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 0.9926 - acc: 0.6390 - val_loss: 0.5368 - val_acc: 0.8000\n",
      "Epoch 25/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9985 - acc: 0.6399Epoch 00024: val_acc improved from 0.80000 to 0.82500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 1.0006 - acc: 0.6392 - val_loss: 0.4774 - val_acc: 0.8250\n",
      "Epoch 26/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9608 - acc: 0.6574Epoch 00025: val_acc improved from 0.82500 to 0.84000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 0.9605 - acc: 0.6575 - val_loss: 0.3870 - val_acc: 0.8400\n",
      "Epoch 27/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9444 - acc: 0.6461Epoch 00026: val_acc improved from 0.84000 to 0.84000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 62s - loss: 0.9447 - acc: 0.6458 - val_loss: 0.3528 - val_acc: 0.8400\n",
      "Epoch 28/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.6630Epoch 00027: val_acc did not improve\n",
      "480/480 [==============================] - 62s - loss: 0.9071 - acc: 0.6627 - val_loss: 0.9956 - val_acc: 0.7150\n",
      "Epoch 29/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.9028 - acc: 0.6660Epoch 00028: val_acc did not improve\n",
      "480/480 [==============================] - 62s - loss: 0.9024 - acc: 0.6665 - val_loss: 0.8281 - val_acc: 0.7450\n",
      "Epoch 30/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.8548 - acc: 0.6908Epoch 00029: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.8556 - acc: 0.6906 - val_loss: 0.7767 - val_acc: 0.7600\n",
      "Epoch 31/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.8621 - acc: 0.6825Epoch 00030: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.8622 - acc: 0.6825 - val_loss: 0.4899 - val_acc: 0.8050\n",
      "Epoch 32/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.8187 - acc: 0.6958Epoch 00031: val_acc improved from 0.84000 to 0.90000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.8201 - acc: 0.6956 - val_loss: 0.2744 - val_acc: 0.9000\n",
      "Epoch 33/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7146Epoch 00032: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7926 - acc: 0.7138 - val_loss: 0.3160 - val_acc: 0.8700\n",
      "Epoch 34/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7872 - acc: 0.7098Epoch 00033: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7874 - acc: 0.7096 - val_loss: 0.4009 - val_acc: 0.8550\n",
      "Epoch 35/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.7161Epoch 00034: val_acc improved from 0.90000 to 0.93500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.7797 - acc: 0.7156 - val_loss: 0.1394 - val_acc: 0.9350\n",
      "Epoch 36/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.7169Epoch 00035: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7722 - acc: 0.7173 - val_loss: 0.8744 - val_acc: 0.7700\n",
      "Epoch 37/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7540 - acc: 0.7280Epoch 00036: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7547 - acc: 0.7277 - val_loss: 0.8639 - val_acc: 0.7350\n",
      "Epoch 38/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7380Epoch 00037: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7132 - acc: 0.7383 - val_loss: 0.2530 - val_acc: 0.8950\n",
      "Epoch 39/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7056 - acc: 0.7409Epoch 00038: val_acc improved from 0.93500 to 0.93500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.7057 - acc: 0.7410 - val_loss: 0.1415 - val_acc: 0.9350\n",
      "Epoch 40/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.7174 - acc: 0.7413Epoch 00039: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.7171 - acc: 0.7413 - val_loss: 0.1810 - val_acc: 0.9200\n",
      "Epoch 41/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6752 - acc: 0.7520Epoch 00040: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6749 - acc: 0.7521 - val_loss: 0.1580 - val_acc: 0.9100\n",
      "Epoch 42/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6784 - acc: 0.7511Epoch 00041: val_acc improved from 0.93500 to 0.94500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.6782 - acc: 0.7513 - val_loss: 0.1054 - val_acc: 0.9450\n",
      "Epoch 43/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.7587Epoch 00042: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6594 - acc: 0.7583 - val_loss: 0.0936 - val_acc: 0.9400\n",
      "Epoch 44/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6704 - acc: 0.7511Epoch 00043: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6698 - acc: 0.7513 - val_loss: 0.1679 - val_acc: 0.9250\n",
      "Epoch 45/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.7580Epoch 00044: val_acc improved from 0.94500 to 0.97500, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.6711 - acc: 0.7579 - val_loss: 0.0873 - val_acc: 0.9750\n",
      "Epoch 46/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.7633Epoch 00045: val_acc improved from 0.97500 to 0.98000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.6442 - acc: 0.7635 - val_loss: 0.0845 - val_acc: 0.9800\n",
      "Epoch 47/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6268 - acc: 0.7706Epoch 00046: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6292 - acc: 0.7698 - val_loss: 0.2330 - val_acc: 0.9050\n",
      "Epoch 48/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.7687Epoch 00047: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6277 - acc: 0.7688 - val_loss: 0.0742 - val_acc: 0.9700\n",
      "Epoch 49/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6202 - acc: 0.7653Epoch 00048: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6198 - acc: 0.7654 - val_loss: 0.2382 - val_acc: 0.9100\n",
      "Epoch 50/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6427 - acc: 0.7699Epoch 00049: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6433 - acc: 0.7696 - val_loss: 0.1255 - val_acc: 0.9450\n",
      "Epoch 51/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.7716Epoch 00050: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6195 - acc: 0.7717 - val_loss: 0.2115 - val_acc: 0.9050\n",
      "Epoch 52/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.7720Epoch 00051: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.6005 - acc: 0.7723 - val_loss: 0.0977 - val_acc: 0.9500\n",
      "Epoch 53/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.7846Epoch 00052: val_acc improved from 0.98000 to 1.00000, saving model to ASL-weights-data_aug-25jul.h5\n",
      "480/480 [==============================] - 61s - loss: 0.6002 - acc: 0.7842 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 54/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.7852Epoch 00053: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5797 - acc: 0.7852 - val_loss: 0.3046 - val_acc: 0.9050\n",
      "Epoch 55/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5809 - acc: 0.7891Epoch 00054: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5818 - acc: 0.7890 - val_loss: 0.0561 - val_acc: 0.9750\n",
      "Epoch 56/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.7866Epoch 00055: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5819 - acc: 0.7863 - val_loss: 0.0673 - val_acc: 0.9650\n",
      "Epoch 57/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7923Epoch 00056: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5685 - acc: 0.7925 - val_loss: 0.1122 - val_acc: 0.9500\n",
      "Epoch 58/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.7904Epoch 00057: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5775 - acc: 0.7906 - val_loss: 0.0707 - val_acc: 0.9600\n",
      "Epoch 59/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.7910Epoch 00058: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5663 - acc: 0.7913 - val_loss: 0.0610 - val_acc: 0.9850\n",
      "Epoch 60/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7931Epoch 00059: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5513 - acc: 0.7931 - val_loss: 0.4117 - val_acc: 0.8850\n",
      "Epoch 61/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.8048Epoch 00060: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5493 - acc: 0.8044 - val_loss: 0.0430 - val_acc: 0.9900\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/480 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8006Epoch 00061: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5393 - acc: 0.8008 - val_loss: 0.0910 - val_acc: 0.9600\n",
      "Epoch 63/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8013Epoch 00062: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5343 - acc: 0.8015 - val_loss: 0.0426 - val_acc: 0.9850\n",
      "Epoch 64/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.8019Epoch 00063: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5479 - acc: 0.8019 - val_loss: 0.0609 - val_acc: 0.9800\n",
      "Epoch 65/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.7958Epoch 00064: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5712 - acc: 0.7960 - val_loss: 0.0526 - val_acc: 0.9850\n",
      "Epoch 66/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8029Epoch 00065: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5355 - acc: 0.8029 - val_loss: 0.1120 - val_acc: 0.9400\n",
      "Epoch 67/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8100Epoch 00066: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5154 - acc: 0.8096 - val_loss: 0.0370 - val_acc: 0.9850\n",
      "Epoch 68/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8150Epoch 00067: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5146 - acc: 0.8150 - val_loss: 0.0244 - val_acc: 0.9950\n",
      "Epoch 69/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8025Epoch 00068: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5321 - acc: 0.8021 - val_loss: 0.0293 - val_acc: 0.9950\n",
      "Epoch 70/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.8027Epoch 00069: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5406 - acc: 0.8029 - val_loss: 0.0689 - val_acc: 0.9700\n",
      "Epoch 71/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.8109Epoch 00070: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5185 - acc: 0.8110 - val_loss: 0.3586 - val_acc: 0.9100\n",
      "Epoch 72/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8125Epoch 00071: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5205 - acc: 0.8123 - val_loss: 0.0748 - val_acc: 0.9750\n",
      "Epoch 73/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8142Epoch 00072: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5139 - acc: 0.8140 - val_loss: 0.0541 - val_acc: 0.9750\n",
      "Epoch 74/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8050Epoch 00073: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5149 - acc: 0.8054 - val_loss: 0.0878 - val_acc: 0.9500\n",
      "Epoch 75/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8163Epoch 00074: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4933 - acc: 0.8167 - val_loss: 0.0675 - val_acc: 0.9700\n",
      "Epoch 76/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8109Epoch 00075: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5243 - acc: 0.8110 - val_loss: 0.1001 - val_acc: 0.9500\n",
      "Epoch 77/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8144Epoch 00076: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5053 - acc: 0.8146 - val_loss: 0.0346 - val_acc: 0.9950\n",
      "Epoch 78/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8225Epoch 00077: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4950 - acc: 0.8227 - val_loss: 0.0524 - val_acc: 0.9750\n",
      "Epoch 79/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8109Epoch 00078: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5193 - acc: 0.8104 - val_loss: 0.0301 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8186Epoch 00079: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5029 - acc: 0.8185 - val_loss: 0.1884 - val_acc: 0.9450\n",
      "Epoch 81/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.8163Epoch 00080: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5058 - acc: 0.8158 - val_loss: 0.0325 - val_acc: 0.9850\n",
      "Epoch 82/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.8244Epoch 00081: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4812 - acc: 0.8244 - val_loss: 0.0437 - val_acc: 0.9750\n",
      "Epoch 83/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8111Epoch 00082: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.5024 - acc: 0.8110 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8217Epoch 00083: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4944 - acc: 0.8221 - val_loss: 0.0257 - val_acc: 0.9900\n",
      "Epoch 85/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8225Epoch 00084: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4924 - acc: 0.8227 - val_loss: 0.0558 - val_acc: 0.9750\n",
      "Epoch 86/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4888 - acc: 0.8232Epoch 00085: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4888 - acc: 0.8231 - val_loss: 0.0255 - val_acc: 0.9800\n",
      "Epoch 87/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8177Epoch 00086: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4980 - acc: 0.8179 - val_loss: 0.0194 - val_acc: 0.9950\n",
      "Epoch 88/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8282Epoch 00087: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4747 - acc: 0.8277 - val_loss: 0.0329 - val_acc: 0.9850\n",
      "Epoch 89/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4559 - acc: 0.8330Epoch 00088: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4555 - acc: 0.8333 - val_loss: 0.0675 - val_acc: 0.9700\n",
      "Epoch 90/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4901 - acc: 0.8238Epoch 00089: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4910 - acc: 0.8235 - val_loss: 0.0158 - val_acc: 0.9950\n",
      "Epoch 91/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8305Epoch 00090: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4668 - acc: 0.8306 - val_loss: 0.0572 - val_acc: 0.9800\n",
      "Epoch 92/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8294Epoch 00091: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4675 - acc: 0.8290 - val_loss: 0.1319 - val_acc: 0.9600\n",
      "Epoch 93/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8351Epoch 00092: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4454 - acc: 0.8350 - val_loss: 0.0223 - val_acc: 0.9850\n",
      "Epoch 94/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.8311Epoch 00093: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4665 - acc: 0.8313 - val_loss: 0.0177 - val_acc: 0.9950\n",
      "Epoch 95/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8261Epoch 00094: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4711 - acc: 0.8263 - val_loss: 0.0410 - val_acc: 0.9650\n",
      "Epoch 96/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4524 - acc: 0.8359Epoch 00095: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4523 - acc: 0.8360 - val_loss: 0.0819 - val_acc: 0.9450\n",
      "Epoch 97/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8363Epoch 00096: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4545 - acc: 0.8367 - val_loss: 0.0738 - val_acc: 0.9700\n",
      "Epoch 98/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8242Epoch 00097: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4604 - acc: 0.8244 - val_loss: 0.0772 - val_acc: 0.9800\n",
      "Epoch 99/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8324Epoch 00098: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4578 - acc: 0.8325 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8401Epoch 00099: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4549 - acc: 0.8400 - val_loss: 0.1020 - val_acc: 0.9750\n",
      "Epoch 101/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8399Epoch 00100: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4362 - acc: 0.8400 - val_loss: 0.0588 - val_acc: 0.9800\n",
      "Epoch 102/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8372Epoch 00101: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4514 - acc: 0.8371 - val_loss: 0.0854 - val_acc: 0.9750\n",
      "Epoch 103/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4629 - acc: 0.8296Epoch 00102: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4631 - acc: 0.8294 - val_loss: 0.0291 - val_acc: 0.9900\n",
      "Epoch 104/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8292Epoch 00103: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4579 - acc: 0.8292 - val_loss: 0.1748 - val_acc: 0.9400\n",
      "Epoch 105/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8372Epoch 00104: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4326 - acc: 0.8371 - val_loss: 0.0383 - val_acc: 0.9850\n",
      "Epoch 106/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8420Epoch 00105: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4417 - acc: 0.8419 - val_loss: 0.0749 - val_acc: 0.9650\n",
      "Epoch 107/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8361Epoch 00106: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4392 - acc: 0.8363 - val_loss: 0.1257 - val_acc: 0.9450\n",
      "Epoch 108/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8313Epoch 00107: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4560 - acc: 0.8317 - val_loss: 0.0816 - val_acc: 0.9650\n",
      "Epoch 109/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8420Epoch 00108: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4417 - acc: 0.8421 - val_loss: 0.1223 - val_acc: 0.9650\n",
      "Epoch 110/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8426Epoch 00109: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4260 - acc: 0.8427 - val_loss: 0.0533 - val_acc: 0.9850\n",
      "Epoch 111/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4359 - acc: 0.8447Epoch 00110: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4357 - acc: 0.8446 - val_loss: 0.1654 - val_acc: 0.9400\n",
      "Epoch 112/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8451Epoch 00111: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4247 - acc: 0.8452 - val_loss: 0.0257 - val_acc: 0.9950\n",
      "Epoch 113/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8443Epoch 00112: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4341 - acc: 0.8444 - val_loss: 0.3326 - val_acc: 0.9350\n",
      "Epoch 114/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8466Epoch 00113: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4198 - acc: 0.8469 - val_loss: 0.0391 - val_acc: 0.9900\n",
      "Epoch 115/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8457Epoch 00114: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4329 - acc: 0.8460 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8505Epoch 00115: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4235 - acc: 0.8504 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8405Epoch 00116: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4347 - acc: 0.8408 - val_loss: 0.0102 - val_acc: 0.9950\n",
      "Epoch 118/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.8420Epoch 00117: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4394 - acc: 0.8419 - val_loss: 0.0418 - val_acc: 0.9850\n",
      "Epoch 119/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8390Epoch 00118: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4314 - acc: 0.8392 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 120/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8468Epoch 00119: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4141 - acc: 0.8467 - val_loss: 0.0636 - val_acc: 0.9800\n",
      "Epoch 121/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8591Epoch 00120: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3954 - acc: 0.8590 - val_loss: 0.0837 - val_acc: 0.9800\n",
      "Epoch 122/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8566Epoch 00121: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4004 - acc: 0.8565 - val_loss: 0.0161 - val_acc: 0.9950\n",
      "Epoch 123/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8424Epoch 00122: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4280 - acc: 0.8423 - val_loss: 0.0389 - val_acc: 0.9800\n",
      "Epoch 124/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8555Epoch 00123: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4065 - acc: 0.8556 - val_loss: 0.1919 - val_acc: 0.9500\n",
      "Epoch 125/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8520Epoch 00124: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4113 - acc: 0.8521 - val_loss: 0.3141 - val_acc: 0.9100\n",
      "Epoch 126/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8564Epoch 00125: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4112 - acc: 0.8560 - val_loss: 0.0181 - val_acc: 0.9900\n",
      "Epoch 127/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8418Epoch 00126: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4255 - acc: 0.8417 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8524Epoch 00127: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4000 - acc: 0.8521 - val_loss: 0.0478 - val_acc: 0.9750\n",
      "Epoch 129/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/480 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8482Epoch 00128: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4178 - acc: 0.8481 - val_loss: 0.1152 - val_acc: 0.9700\n",
      "Epoch 130/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8447Epoch 00129: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4238 - acc: 0.8446 - val_loss: 0.0258 - val_acc: 0.9900\n",
      "Epoch 131/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8522Epoch 00130: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4152 - acc: 0.8521 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8555Epoch 00131: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4035 - acc: 0.8556 - val_loss: 0.0123 - val_acc: 0.9950\n",
      "Epoch 133/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8466Epoch 00132: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4206 - acc: 0.8460 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8426Epoch 00133: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4226 - acc: 0.8429 - val_loss: 0.0134 - val_acc: 0.9950\n",
      "Epoch 135/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8480Epoch 00134: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4137 - acc: 0.8483 - val_loss: 0.0131 - val_acc: 0.9950\n",
      "Epoch 136/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8486Epoch 00135: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4146 - acc: 0.8485 - val_loss: 0.0873 - val_acc: 0.9750\n",
      "Epoch 137/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8578Epoch 00136: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3975 - acc: 0.8579 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8495Epoch 00137: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4204 - acc: 0.8498 - val_loss: 0.0152 - val_acc: 0.9950\n",
      "Epoch 139/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8576Epoch 00138: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3941 - acc: 0.8575 - val_loss: 0.0562 - val_acc: 0.9850\n",
      "Epoch 140/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8653Epoch 00139: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3831 - acc: 0.8652 - val_loss: 0.0165 - val_acc: 0.9950\n",
      "Epoch 141/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8578Epoch 00140: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4000 - acc: 0.8579 - val_loss: 0.0608 - val_acc: 0.9800\n",
      "Epoch 142/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8524Epoch 00141: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4081 - acc: 0.8523 - val_loss: 0.0374 - val_acc: 0.9850\n",
      "Epoch 143/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8559Epoch 00142: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4041 - acc: 0.8560 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8522Epoch 00143: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4023 - acc: 0.8523 - val_loss: 0.0996 - val_acc: 0.9750\n",
      "Epoch 145/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8537Epoch 00144: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3997 - acc: 0.8537 - val_loss: 0.0176 - val_acc: 0.9950\n",
      "Epoch 146/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8591Epoch 00145: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3857 - acc: 0.8588 - val_loss: 0.0164 - val_acc: 0.9950\n",
      "Epoch 147/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8516Epoch 00146: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4036 - acc: 0.8519 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8528Epoch 00147: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4143 - acc: 0.8527 - val_loss: 0.0152 - val_acc: 0.9900\n",
      "Epoch 149/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8432Epoch 00148: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4179 - acc: 0.8427 - val_loss: 0.0235 - val_acc: 0.9900\n",
      "Epoch 150/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8576Epoch 00149: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3860 - acc: 0.8579 - val_loss: 0.1038 - val_acc: 0.9750\n",
      "Epoch 151/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8580Epoch 00150: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4074 - acc: 0.8579 - val_loss: 0.0108 - val_acc: 0.9900\n",
      "Epoch 152/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8551Epoch 00151: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4117 - acc: 0.8552 - val_loss: 0.0315 - val_acc: 0.9850\n",
      "Epoch 153/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8407Epoch 00152: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4371 - acc: 0.8406 - val_loss: 0.0728 - val_acc: 0.9800\n",
      "Epoch 154/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8511Epoch 00153: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.4032 - acc: 0.8515 - val_loss: 0.0708 - val_acc: 0.9750\n",
      "Epoch 155/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8566Epoch 00154: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3938 - acc: 0.8560 - val_loss: 0.0718 - val_acc: 0.9800\n",
      "Epoch 156/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8639Epoch 00155: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3851 - acc: 0.8637 - val_loss: 0.1963 - val_acc: 0.9400\n",
      "Epoch 157/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8576Epoch 00156: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3974 - acc: 0.8573 - val_loss: 0.0333 - val_acc: 0.9850\n",
      "Epoch 158/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8589Epoch 00157: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3879 - acc: 0.8588 - val_loss: 0.0917 - val_acc: 0.9700\n",
      "Epoch 159/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8628Epoch 00158: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3810 - acc: 0.8627 - val_loss: 0.0183 - val_acc: 0.9950\n",
      "Epoch 160/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8622Epoch 00159: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3831 - acc: 0.8621 - val_loss: 0.0272 - val_acc: 0.9800\n",
      "Epoch 161/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8605Epoch 00160: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3844 - acc: 0.8604 - val_loss: 0.0611 - val_acc: 0.9750\n",
      "Epoch 162/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8589Epoch 00161: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3891 - acc: 0.8592 - val_loss: 0.0991 - val_acc: 0.9700\n",
      "Epoch 163/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8614Epoch 00162: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3891 - acc: 0.8615 - val_loss: 0.0181 - val_acc: 0.9900\n",
      "Epoch 164/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8574Epoch 00163: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3948 - acc: 0.8573 - val_loss: 0.0206 - val_acc: 0.9900\n",
      "Epoch 165/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8599Epoch 00164: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3872 - acc: 0.8602 - val_loss: 0.0271 - val_acc: 0.9900\n",
      "Epoch 166/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8678Epoch 00165: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3785 - acc: 0.8679 - val_loss: 0.0788 - val_acc: 0.9650\n",
      "Epoch 167/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8587Epoch 00166: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3869 - acc: 0.8585 - val_loss: 0.0812 - val_acc: 0.9600\n",
      "Epoch 168/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8595Epoch 00167: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3903 - acc: 0.8594 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8622Epoch 00168: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3786 - acc: 0.8623 - val_loss: 0.0218 - val_acc: 0.9900\n",
      "Epoch 170/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8649Epoch 00169: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3792 - acc: 0.8650 - val_loss: 0.0375 - val_acc: 0.9900\n",
      "Epoch 171/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8582Epoch 00170: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3885 - acc: 0.8583 - val_loss: 0.0987 - val_acc: 0.9750\n",
      "Epoch 172/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8633Epoch 00171: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3800 - acc: 0.8631 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8666Epoch 00172: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3710 - acc: 0.8667 - val_loss: 0.0397 - val_acc: 0.9850\n",
      "Epoch 174/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8658Epoch 00173: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3683 - acc: 0.8658 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8578Epoch 00174: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3806 - acc: 0.8579 - val_loss: 0.0268 - val_acc: 0.9800\n",
      "Epoch 176/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8608Epoch 00175: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3747 - acc: 0.8610 - val_loss: 0.0369 - val_acc: 0.9850\n",
      "Epoch 177/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8608Epoch 00176: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3745 - acc: 0.8606 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 178/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8681Epoch 00177: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3559 - acc: 0.8681 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8656Epoch 00178: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3739 - acc: 0.8658 - val_loss: 0.0316 - val_acc: 0.9800\n",
      "Epoch 180/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8572Epoch 00179: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3815 - acc: 0.8573 - val_loss: 0.0573 - val_acc: 0.9800\n",
      "Epoch 181/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8656Epoch 00180: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3674 - acc: 0.8656 - val_loss: 0.0993 - val_acc: 0.9750\n",
      "Epoch 182/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8699Epoch 00181: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3629 - acc: 0.8702 - val_loss: 0.1063 - val_acc: 0.9600\n",
      "Epoch 183/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8676Epoch 00182: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3689 - acc: 0.8677 - val_loss: 0.0589 - val_acc: 0.9750\n",
      "Epoch 184/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8587Epoch 00183: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3754 - acc: 0.8585 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8645Epoch 00184: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3759 - acc: 0.8648 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 186/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8610Epoch 00185: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3794 - acc: 0.8613 - val_loss: 0.1121 - val_acc: 0.9650\n",
      "Epoch 187/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3686 - acc: 0.8666Epoch 00186: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3688 - acc: 0.8665 - val_loss: 0.0247 - val_acc: 0.9850\n",
      "Epoch 188/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8664Epoch 00187: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3678 - acc: 0.8662 - val_loss: 0.0163 - val_acc: 0.9900\n",
      "Epoch 189/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8681Epoch 00188: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3649 - acc: 0.8679 - val_loss: 0.0175 - val_acc: 0.9900\n",
      "Epoch 190/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8722Epoch 00189: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3460 - acc: 0.8725 - val_loss: 0.0471 - val_acc: 0.9700\n",
      "Epoch 191/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8622Epoch 00190: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3803 - acc: 0.8625 - val_loss: 0.0355 - val_acc: 0.9800\n",
      "Epoch 192/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8745Epoch 00191: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3530 - acc: 0.8748 - val_loss: 0.0320 - val_acc: 0.9850\n",
      "Epoch 193/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8622Epoch 00192: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3691 - acc: 0.8621 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8599Epoch 00193: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3716 - acc: 0.8598 - val_loss: 0.0114 - val_acc: 0.9950\n",
      "Epoch 195/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8678Epoch 00194: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3683 - acc: 0.8679 - val_loss: 0.0263 - val_acc: 0.9900\n",
      "Epoch 196/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/480 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8691Epoch 00195: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3598 - acc: 0.8690 - val_loss: 0.0153 - val_acc: 0.9950\n",
      "Epoch 197/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8628Epoch 00196: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3648 - acc: 0.8629 - val_loss: 0.0203 - val_acc: 0.9950\n",
      "Epoch 198/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8641Epoch 00197: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3685 - acc: 0.8640 - val_loss: 0.0191 - val_acc: 0.9900\n",
      "Epoch 199/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8633Epoch 00198: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3662 - acc: 0.8633 - val_loss: 0.0311 - val_acc: 0.9800\n",
      "Epoch 200/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8656Epoch 00199: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3686 - acc: 0.8654 - val_loss: 0.0774 - val_acc: 0.9750\n",
      "Epoch 201/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8614Epoch 00200: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3748 - acc: 0.8615 - val_loss: 0.0255 - val_acc: 0.9850\n",
      "Epoch 202/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8689Epoch 00201: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3730 - acc: 0.8690 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8649Epoch 00202: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3722 - acc: 0.8648 - val_loss: 0.0147 - val_acc: 0.9950\n",
      "Epoch 204/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8689Epoch 00203: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3614 - acc: 0.8687 - val_loss: 0.0718 - val_acc: 0.9850\n",
      "Epoch 205/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8706Epoch 00204: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3550 - acc: 0.8708 - val_loss: 0.1176 - val_acc: 0.9700\n",
      "Epoch 206/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8762Epoch 00205: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3427 - acc: 0.8762 - val_loss: 0.0159 - val_acc: 0.9950\n",
      "Epoch 207/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8674Epoch 00206: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3591 - acc: 0.8677 - val_loss: 0.0570 - val_acc: 0.9850\n",
      "Epoch 208/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8808Epoch 00207: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3352 - acc: 0.8808 - val_loss: 0.0400 - val_acc: 0.9800\n",
      "Epoch 209/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8645Epoch 00208: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3620 - acc: 0.8648 - val_loss: 0.0470 - val_acc: 0.9700\n",
      "Epoch 210/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8643Epoch 00209: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3828 - acc: 0.8644 - val_loss: 0.0287 - val_acc: 0.9900\n",
      "Epoch 211/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8664Epoch 00210: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3797 - acc: 0.8663 - val_loss: 0.0297 - val_acc: 0.9900\n",
      "Epoch 212/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8743Epoch 00211: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3479 - acc: 0.8740 - val_loss: 0.0224 - val_acc: 0.9950\n",
      "Epoch 213/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8664Epoch 00212: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3642 - acc: 0.8667 - val_loss: 0.0802 - val_acc: 0.9700\n",
      "Epoch 214/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8810Epoch 00213: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3419 - acc: 0.8810 - val_loss: 0.0877 - val_acc: 0.9750\n",
      "Epoch 215/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8745Epoch 00214: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3513 - acc: 0.8744 - val_loss: 0.1822 - val_acc: 0.9650\n",
      "Epoch 216/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8745Epoch 00215: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3523 - acc: 0.8746 - val_loss: 0.1236 - val_acc: 0.9700\n",
      "Epoch 217/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8685Epoch 00216: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3684 - acc: 0.8679 - val_loss: 0.1109 - val_acc: 0.9650\n",
      "Epoch 218/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8777Epoch 00217: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3611 - acc: 0.8777 - val_loss: 0.0922 - val_acc: 0.9750\n",
      "Epoch 219/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8727Epoch 00218: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3436 - acc: 0.8727 - val_loss: 0.0263 - val_acc: 0.9850\n",
      "Epoch 220/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8777Epoch 00219: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3398 - acc: 0.8775 - val_loss: 0.0607 - val_acc: 0.9800\n",
      "Epoch 221/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8612Epoch 00220: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3789 - acc: 0.8612 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 222/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8745Epoch 00221: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3406 - acc: 0.8746 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8693Epoch 00222: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3640 - acc: 0.8690 - val_loss: 0.0446 - val_acc: 0.9800\n",
      "Epoch 224/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8683Epoch 00223: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3604 - acc: 0.8679 - val_loss: 0.0314 - val_acc: 0.9850\n",
      "Epoch 225/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8727Epoch 00224: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3635 - acc: 0.8727 - val_loss: 0.0779 - val_acc: 0.9800\n",
      "Epoch 226/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8718Epoch 00225: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3567 - acc: 0.8717 - val_loss: 0.0098 - val_acc: 0.9950\n",
      "Epoch 227/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3419 - acc: 0.8733Epoch 00226: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3416 - acc: 0.8733 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "Epoch 228/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8624Epoch 00227: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3700 - acc: 0.8627 - val_loss: 0.1294 - val_acc: 0.9600\n",
      "Epoch 229/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8766Epoch 00228: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3489 - acc: 0.8769 - val_loss: 0.0553 - val_acc: 0.9700\n",
      "Epoch 230/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8785Epoch 00229: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3419 - acc: 0.8785 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8739Epoch 00230: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3446 - acc: 0.8737 - val_loss: 0.0078 - val_acc: 0.9950\n",
      "Epoch 232/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8772Epoch 00231: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3417 - acc: 0.8773 - val_loss: 0.0147 - val_acc: 0.9950\n",
      "Epoch 233/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8777Epoch 00232: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3401 - acc: 0.8775 - val_loss: 0.0571 - val_acc: 0.9750\n",
      "Epoch 234/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8689Epoch 00233: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3741 - acc: 0.8687 - val_loss: 0.0599 - val_acc: 0.9750\n",
      "Epoch 235/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.8800Epoch 00234: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3357 - acc: 0.8800 - val_loss: 0.1548 - val_acc: 0.9550\n",
      "Epoch 236/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8747Epoch 00235: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3469 - acc: 0.8748 - val_loss: 0.0690 - val_acc: 0.9700\n",
      "Epoch 237/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8737Epoch 00236: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3386 - acc: 0.8740 - val_loss: 0.1472 - val_acc: 0.9550\n",
      "Epoch 238/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8701Epoch 00237: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3652 - acc: 0.8702 - val_loss: 0.0487 - val_acc: 0.9800\n",
      "Epoch 239/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8783Epoch 00238: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3415 - acc: 0.8785 - val_loss: 0.0440 - val_acc: 0.9850\n",
      "Epoch 240/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8689Epoch 00239: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3625 - acc: 0.8692 - val_loss: 0.3508 - val_acc: 0.9100\n",
      "Epoch 241/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8745Epoch 00240: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3491 - acc: 0.8746 - val_loss: 0.3011 - val_acc: 0.9300\n",
      "Epoch 242/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8724Epoch 00241: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3623 - acc: 0.8727 - val_loss: 0.1481 - val_acc: 0.9700\n",
      "Epoch 243/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8770Epoch 00242: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3443 - acc: 0.8771 - val_loss: 0.0972 - val_acc: 0.9600\n",
      "Epoch 244/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8664Epoch 00243: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3627 - acc: 0.8665 - val_loss: 0.0172 - val_acc: 0.9950\n",
      "Epoch 245/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8800Epoch 00244: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3240 - acc: 0.8802 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 246/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8772Epoch 00245: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3330 - acc: 0.8773 - val_loss: 0.0256 - val_acc: 0.9900\n",
      "Epoch 247/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8743Epoch 00246: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3522 - acc: 0.8744 - val_loss: 0.0604 - val_acc: 0.9650\n",
      "Epoch 248/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8716Epoch 00247: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3550 - acc: 0.8712 - val_loss: 0.0358 - val_acc: 0.9800\n",
      "Epoch 249/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8697Epoch 00248: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3601 - acc: 0.8698 - val_loss: 0.0173 - val_acc: 0.9950\n",
      "Epoch 250/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8745Epoch 00249: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3547 - acc: 0.8746 - val_loss: 0.0644 - val_acc: 0.9750\n",
      "Epoch 251/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8691Epoch 00250: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3546 - acc: 0.8694 - val_loss: 0.1383 - val_acc: 0.9550\n",
      "Epoch 252/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8841Epoch 00251: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3356 - acc: 0.8844 - val_loss: 0.0232 - val_acc: 0.9900\n",
      "Epoch 253/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8708Epoch 00252: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3547 - acc: 0.8708 - val_loss: 0.0493 - val_acc: 0.9800\n",
      "Epoch 254/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8653Epoch 00253: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3561 - acc: 0.8654 - val_loss: 0.0103 - val_acc: 0.9950\n",
      "Epoch 255/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8779Epoch 00254: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3347 - acc: 0.8779 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8779Epoch 00255: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3475 - acc: 0.8781 - val_loss: 0.1181 - val_acc: 0.9650\n",
      "Epoch 257/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8699Epoch 00256: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3579 - acc: 0.8702 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8783Epoch 00257: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3334 - acc: 0.8781 - val_loss: 0.1297 - val_acc: 0.9600\n",
      "Epoch 259/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8733Epoch 00258: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3345 - acc: 0.8733 - val_loss: 0.0481 - val_acc: 0.9650\n",
      "Epoch 260/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8756Epoch 00259: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3463 - acc: 0.8752 - val_loss: 0.0468 - val_acc: 0.9800\n",
      "Epoch 261/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8731Epoch 00260: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3550 - acc: 0.8729 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8777Epoch 00261: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3398 - acc: 0.8775 - val_loss: 0.0536 - val_acc: 0.9850\n",
      "Epoch 263/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/480 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8747Epoch 00262: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3388 - acc: 0.8748 - val_loss: 0.0834 - val_acc: 0.9700\n",
      "Epoch 264/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8789Epoch 00263: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3295 - acc: 0.8790 - val_loss: 0.0141 - val_acc: 0.9950\n",
      "Epoch 265/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8737Epoch 00264: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3547 - acc: 0.8729 - val_loss: 0.0771 - val_acc: 0.9800\n",
      "Epoch 266/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8797Epoch 00265: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3336 - acc: 0.8796 - val_loss: 0.0319 - val_acc: 0.9800\n",
      "Epoch 267/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8795Epoch 00266: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3398 - acc: 0.8798 - val_loss: 0.0522 - val_acc: 0.9800\n",
      "Epoch 268/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.8676Epoch 00267: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3514 - acc: 0.8677 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8720Epoch 00268: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3465 - acc: 0.8715 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "Epoch 270/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8716Epoch 00269: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3493 - acc: 0.8715 - val_loss: 0.1304 - val_acc: 0.9700\n",
      "Epoch 271/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8835Epoch 00270: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3153 - acc: 0.8835 - val_loss: 0.0133 - val_acc: 0.9900\n",
      "Epoch 272/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8814Epoch 00271: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3308 - acc: 0.8817 - val_loss: 0.0403 - val_acc: 0.9850\n",
      "Epoch 273/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8743Epoch 00272: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3439 - acc: 0.8740 - val_loss: 0.0210 - val_acc: 0.9950\n",
      "Epoch 274/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8862Epoch 00273: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3118 - acc: 0.8862 - val_loss: 0.0306 - val_acc: 0.9850\n",
      "Epoch 275/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8697Epoch 00274: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3566 - acc: 0.8700 - val_loss: 0.0376 - val_acc: 0.9800\n",
      "Epoch 276/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8812Epoch 00275: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3253 - acc: 0.8810 - val_loss: 0.0379 - val_acc: 0.9850\n",
      "Epoch 277/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8685Epoch 00276: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3541 - acc: 0.8683 - val_loss: 0.0340 - val_acc: 0.9800\n",
      "Epoch 278/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8762Epoch 00277: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3512 - acc: 0.8758 - val_loss: 0.0217 - val_acc: 0.9950\n",
      "Epoch 279/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8710Epoch 00278: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3511 - acc: 0.8712 - val_loss: 0.0690 - val_acc: 0.9800\n",
      "Epoch 280/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8737Epoch 00279: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3395 - acc: 0.8735 - val_loss: 0.0752 - val_acc: 0.9700\n",
      "Epoch 281/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8806Epoch 00280: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3217 - acc: 0.8806 - val_loss: 0.0575 - val_acc: 0.9750\n",
      "Epoch 282/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8733Epoch 00281: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3449 - acc: 0.8735 - val_loss: 0.1599 - val_acc: 0.9650\n",
      "Epoch 283/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8775Epoch 00282: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3439 - acc: 0.8773 - val_loss: 0.0114 - val_acc: 0.9950\n",
      "Epoch 284/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8804Epoch 00283: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3309 - acc: 0.8806 - val_loss: 0.0306 - val_acc: 0.9900\n",
      "Epoch 285/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8785Epoch 00284: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3390 - acc: 0.8783 - val_loss: 0.0810 - val_acc: 0.9650\n",
      "Epoch 286/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8829Epoch 00285: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3248 - acc: 0.8831 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8858Epoch 00286: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3210 - acc: 0.8860 - val_loss: 0.0125 - val_acc: 0.9950\n",
      "Epoch 288/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8806Epoch 00287: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3290 - acc: 0.8808 - val_loss: 0.0239 - val_acc: 0.9950\n",
      "Epoch 289/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8708Epoch 00288: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3464 - acc: 0.8708 - val_loss: 0.2211 - val_acc: 0.9350\n",
      "Epoch 290/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8816Epoch 00289: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3258 - acc: 0.8815 - val_loss: 0.4840 - val_acc: 0.9150\n",
      "Epoch 291/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8781Epoch 00290: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3409 - acc: 0.8779 - val_loss: 0.0804 - val_acc: 0.9850\n",
      "Epoch 292/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8837Epoch 00291: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3242 - acc: 0.8833 - val_loss: 0.0231 - val_acc: 0.9850\n",
      "Epoch 293/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8823Epoch 00292: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3230 - acc: 0.8821 - val_loss: 0.0651 - val_acc: 0.9850\n",
      "Epoch 294/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8841Epoch 00293: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3313 - acc: 0.8840 - val_loss: 0.0252 - val_acc: 0.9900\n",
      "Epoch 295/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8837Epoch 00294: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3297 - acc: 0.8833 - val_loss: 0.0330 - val_acc: 0.9900\n",
      "Epoch 296/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8810Epoch 00295: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3355 - acc: 0.8810 - val_loss: 0.0596 - val_acc: 0.9850\n",
      "Epoch 297/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8718Epoch 00296: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3468 - acc: 0.8717 - val_loss: 0.0253 - val_acc: 0.9800\n",
      "Epoch 298/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8891Epoch 00297: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3141 - acc: 0.8887 - val_loss: 0.0991 - val_acc: 0.9700\n",
      "Epoch 299/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8820Epoch 00298: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3235 - acc: 0.8815 - val_loss: 0.0279 - val_acc: 0.9900\n",
      "Epoch 300/300\n",
      "479/480 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8810Epoch 00299: val_acc did not improve\n",
      "480/480 [==============================] - 61s - loss: 0.3415 - acc: 0.8812 - val_loss: 0.0864 - val_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbce1763940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's make the data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "         rotation_range=20,\n",
    "         width_shift_range=0.4,\n",
    "         height_shift_range=0.4,\n",
    "         zoom_range=.4,\n",
    "         vertical_flip=True,\n",
    "        )\n",
    "\n",
    "\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from os.path import isfile\n",
    "data_aug_weight_file = 'ASL-weights-data_aug-25jul.h5'\n",
    "\n",
    "if (isfile(data_aug_weight_file)):\n",
    "    model.load_weights(data_aug_weight_file)\n",
    "\n",
    "checkpoint = ModelCheckpoint(data_aug_weight_file_new, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir='./logs-dataAug-asl', histogram_freq=0,\n",
    "                          write_graph=True, write_images=True)\n",
    "\n",
    "model.fit_generator(train_datagen.flow(trainX, trainY, batch_size=10),validation_data=(testX,testY),\n",
    "                    steps_per_epoch=len(trainX) /10, epochs=300,callbacks=[checkpoint,tensorboard])\n",
    "                    # Trying to change batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1   2   3   4   5   6   7   8   9\n",
       "row_0                                        \n",
       "0      20   0   0   0   0   0   0   0   0   0\n",
       "1       0  20   0   0   0   0   0   0   0   0\n",
       "2       0   0  20   0   0   0   0   0   0   0\n",
       "3       0   0   0  20   0   0   0   0   0   0\n",
       "4       0   0   0   0  20   0   0   0   0   0\n",
       "5       0   0   0   0   0  20   0   0   0   0\n",
       "6       0   0   0   0   0   0  20   0   0   0\n",
       "7       0   0   0   0   0   0   2  18   0   0\n",
       "8       0   0   0   0   0   0   0   0  20   0\n",
       "9       0   0   0   0   0   0   1   0   0  19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.load_weights('ASL-new-normal-weights.h5')\n",
    "y_test = np.load('ASL_Test.npz')['arr_1']\n",
    "y_test = y_test.astype('int8')\n",
    "y_hat = model.predict_classes(testX\n",
    "                             )\n",
    "pd.crosstab(y_test,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Test Cases: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAB9CAYAAACie9bzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngEbjcrQIaDQuR4uARuNy/h8S1XezjIbYZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcd6fab240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_wrong = [im for im in zip(testX,y_hat,y_test) if im[1] != im[2]]\n",
    "print(\"Wrong Test Cases:\",len(test_wrong))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for ind, val in enumerate(test_wrong[:100]):\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    plt.subplot(10, 10, ind + 1)\n",
    "    im = 1 - val[0].reshape((100,100))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str('Pred: '+str(val[1]))+'\\n'+str('True: '+str(val[2])), fontsize=14, color='black')\n",
    "    plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result\n",
    "\n",
    "Result slows that after training with data augmentation the DNN has a 100% validation accuracy and a 99.9% train accuracy, (which is without augmentation).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4800 [==============================] - 12s    \n",
      "Wrong Test Cases: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAB9CAYAAABJehABAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUANXURRFURRFCRX/AzjsV0pXW/PpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc701beda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_weights('ASL-new-normal-weights.h5')\n",
    "y_test = np.load('ASL_Train.npz')['arr_1']\n",
    "y_test = y_test.astype('int8')\n",
    "y_hat = model.predict_classes(trainX\n",
    "                             )\n",
    "pd.crosstab(y_test,y_hat)\n",
    "test_wrong = [im for im in zip(trainX,y_hat,y_test) if im[1] != im[2]]\n",
    "print(\"Wrong Test Cases:\",len(test_wrong))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for ind, val in enumerate(test_wrong[:100]):\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    plt.subplot(10, 10, ind + 1)\n",
    "    im = 1 - val[0].reshape((100,100))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str('Pred: '+str(val[1]))+'\\n'+str('True: '+str(val[2])), fontsize=14, color='black')\n",
    "    plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
